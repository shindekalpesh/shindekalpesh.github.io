[{"authors":["admin"],"categories":null,"content":"I am an Electrical and Electronics Graduate, currently doing my Master\u0026rsquo;s in Systems Engineering and Engineering Management, with a special focus on applications of Machine Learning in Industrial Automation.\nMy Master Thesis is focussed on developing a novel Regularization Algorithm for Multi-Task Lifelong Learning in Deep Neural Networks. Additionally, I am working as a Research Assistant and have a vast experience in devising algorithms for Condition Monitoring, Predictive Maintenance and Computer Vision.\nI am proficient in Deep Learning, Machine Learning, Data Analysis and Visualization. I am passionate about research involving Deep Learning, Computer Vision and Artificial Intelligence.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://pranaymodukuru.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am an Electrical and Electronics Graduate, currently doing my Master\u0026rsquo;s in Systems Engineering and Engineering Management, with a special focus on applications of Machine Learning in Industrial Automation.\nMy Master Thesis is focussed on developing a novel Regularization Algorithm for Multi-Task Lifelong Learning in Deep Neural Networks. Additionally, I am working as a Research Assistant and have a vast experience in devising algorithms for Condition Monitoring, Predictive Maintenance and Computer Vision.","tags":null,"title":"Naga Sai Pranay Modukuru","type":"authors"},{"authors":["Pranay Modukuru"],"categories":[],"content":"\rView post on Medium.\n\rColumns with more than 30% missing values\r\r\rAfter cleaning the misrecorded values, the next step is to deal with the missing values itself. An analysis of percentage missing values per column is performed to determine how many columns have missing values and if they did contain how much percentage. Figure 1 shows the columns which have more than 30% missing values. After this analysis, a threshold of 30 was selected to drop the columns. Later an analysis on missing values row-wise had to be performed in order to remove observations that have missing features. Here a threshold of 50 missing features per observation was selected to drop rows. After this analysis, the resulting shapes were:\r* General population - (737288x356)\r* Customers data - (13426x356) (neglecting the additional customer specific features)\r#### Feature Engineering\rThere were certain categorical features which were encoded with numeric values (in fact many, but only a few were addressed for simplicity). These features were coded with binary encoding. Also, some features were containing too much information, for example, information about financial status and age in one single column. Such kind of features was identified and was either recoded to contain broader information or divided into two columns to contain both features separately. Any feature that contained more than 20 categories was either dropped or reconstructed into something useful with the help of the Attribute information file.\rSince many of the features contained categorical values dumped into a single column, this step has helped to simplify the data for the later steps. This step resulted in having 353 features.\r#### Imputing Missing Values\rEven after dropping columns and rows based on a certain threshold, we are left with data which still has missing values. This problem is addressed with the help of simple imputer, which fills in the missing data with some values which we can control. A general approach for numerical features would be to impute the missing values with the median or mean. But a more common approach for categorical features is to impute with most common values. Since data corresponds to population, so it is more sensitive to impute the missing values with the most common values.\rNow, the data is clean and ready for modelling, one final step is to scale the data, i.e. to bring all the features to the same range. This is done using a standard scaler.\r## Customer Segmentation using Unsupervised Learning\rFor cluster segmentation there are two steps to be performed.\r* Dimensionality Reduction\r* Clustering\r#### Dimensionality Reduction\rAlthough we have 353 features, not all of them will have variation, i.e. some features might be the same for all the people. We can go through all the features here to see how many unique values are there per feature to select the features which have the required variation. But a more systematic approach would be to perform a certain analysis before dropping any column. Hence, Principal Component Analysis (PCA) has been performed to analyse the explained variance of the PCA components. PCA applies a linear transformation on the data to form a new coordinate system such that the components in the new coordinate system represent the variation in the data.\r\rPCA Explained Variance plot\r\r\rThis analysis will help us determine how many features have enough variance to explain the variation in the data. An explained variance plot is used to select the number of components, i.e. the number of dimensions in the reduced coordinate space. As seen from the above plot, almost 90% of the variance can be explained with the help of approximately 150 components. Now, after the PCA transformation, we are left with 150 PCA components, each made up of a linear combination between the main features.\r#### Clustering\rAfter the dimensionality reduction, the next step is to divide the general population and customer population into different segments. K-Means clustering algorithm has been chosen for this task. Since it is simple and is apt for this task since it measures the distance between two observations to assign a cluster. This algorithm will help us in separating the general population with the help of the reduced features into a specified number of clusters and use this cluster information to understand the similarities in the general population and customer data. The number of clusters is selected to be ‘8’ with the help of an elbow plot.\r#### Cluster Analysis\r\rCluster proportions\r\r\rThe general population and the customer population have been clustered into segments. Figure 3 represents the proportions of the population coming into each cluster. The cluster distribution of the general population is uniform, meaning that the general population has been uniformly clustered into 8 segments. But the customer population seems to be coming from the clusters ‘0’, ‘3’, ‘4’ and ‘7’. We can further confirm this by taking the ratio of proportions of customers segments and general population segments, as shown in Figure 4.\r\rCluster proportions\r\r\rAs seen in Figure 4, if the ratio of proportions is greater than one, that means this cluster has a greater number of customers in the existing population and has the potential to have more future customers. If the ratio is less than one, that means these clusters have the least possibility to have future customers.\rA more detailed cluster analysis, which explains each cluster and corresponding components is also performed. It is documented in the [jupyter notebook](https://github.com/pranaymodukuru/Bertelsmann-Arvato-customer-segmentation/blob/master/Arvato%20Project%20Workbook.ipynb) used for this project. I am not explaining it here as this blog post is already too long.\r## Customer Acquisition using Supervised Learning\rAfter analysing the general population and customers data understanding which segments to concentrate on. We can further extend this analysis to make use of ML algorithms to take this decision too. Since we already have the customers data and general population data, we can combine them to form training data and train the ML models to make predictions about whether to approach a customer or not.\rIn this case, supervised learning is done with the given train and test data. AUROC score has been selected as the evaluation metric since the problem is a highly imbalanced classification. The baseline performance was set with a Logistic Regression model, which was further improved with the help of Tree-based ensemble models. The AdaboostClassifier and XGBoostClassifier were the final selected models, whose predictions were submitted to Kaggle to attain a position in top 30 percentile (on the date of submission) with only two submissions.\r\rKaggle Leaderboard\r\r\r## Conclusion\rThe general population and customer population have been compared and segmented using an Unsupervised learning algorithm. We were able to determine which clusters have more customers and which are potential clusters to have probable customers. We have also used supervised learning algorithms to predict a possible future customer based on demographic data.\rThe resulting analysis has produced good results to put me in the top 30 percentile in the competition Leader board. The top score in the Leaderboard is 0.81063, which is not far away from the score that I achieved (0.80027). There is scope for improvement in the data preparation steps.\rA more comprehensive explanation of each step and the reasons behind choices of algorithms and metrics has been given in the [project report](https://github.com/pranaymodukuru/Bertelsmann-Arvato-customer-segmentation/blob/master/Report.pdf) and all the steps are documented in this [notebook](https://github.com/pranaymodukuru/Bertelsmann-Arvato-customer-segmentation/blob/master/Arvato%20Project%20Workbook.ipynb).\rFinally, I would like to thank Arvato Financial Solutions and Udacity for providing this wonderful opportunity to work with real-world data. This helped me gain valuable experience and helped me use and improve my skills.\r## References\r1. https://www.business2community.com/customer-experience/4-types-of-customer-segmentation-all-marketers-should-know-02120397\r2. https://blog.alexa.com/types-of-market-segmentation/\r3. https://clevertap.com/blog/customer-segmentation-examples-for-better-mobile-marketing/\r4. https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/\r5. https://towardsdatascience.com/what-metrics-should-we-use-on-imbalanced-data-set-precision-recall-roc-e2e79252aeba --\r","date":1587048357,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587048357,"objectID":"0a43c77d8a4c34b4ed680ba3c9225980","permalink":"https://pranaymodukuru.github.io/post/customer-segmentation/","publishdate":"2020-04-16T16:45:57+02:00","relpermalink":"/post/customer-segmentation/","section":"post","summary":"Using Unsupervised Learning to Cluster Customers into segments based on Demographic data and Supervised Learning to predict potential customers","tags":["Machine Learning","Data Science"],"title":"Customer Segmentation and Acquisition using Machine Learning","type":"post"},{"authors":[],"categories":[],"content":"Project Overview In this project, the demographic data of German population and the customer data have been analysed in order to perform Customer Segmentation and Customer Acquisition. Arvato Financial Solutions is a services company that provides financial services, Information Technology (IT) services and Supply Chain Management (SCM) solutions for business customers on a global scale.\nThis project is to help a Mail-Order company to acquire new customers to sell its organic products. The goal of this project is to understand the customer demographics as compared to general population in order to decide whether to approach a person for future products.\nPlease click on links below for more details  \rProject \rCode  ","date":1586775014,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586775014,"objectID":"a786b7a2a15c1bd7d416cd5a390881b5","permalink":"https://pranaymodukuru.github.io/project/customer-segmentation-acquisition/","publishdate":"2020-04-13T12:50:14+02:00","relpermalink":"/project/customer-segmentation-acquisition/","section":"project","summary":"Using Unsupervised and Supervised Learning Techniques","tags":["Machine Learning","Data Science"],"title":"Customer Segmentation and Acquisition","type":"project"},{"authors":null,"categories":null,"content":"Concrete Compressive Strength Prediction using Machine Learning Concrete is one of the most important materials in Civil Engineering. Knowing the compressive strength of concrete is very important when constructing a building or a bridge. The Compressive Strength of Concrete is a highly nonlinear function of ingredients used in making it and their characteristics. Thus, using Machine Learning to predict the Strength could be useful in generating a combination of ingredients which result in high Strength.\nPlease click on links below for more details  \rProject \rCode Medium Blog - Also leave some claps to show appreciation!  ","date":1583712000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583712000,"objectID":"f5b973531a5210f476680886003660f3","permalink":"https://pranaymodukuru.github.io/project/concrete-compressive-strength/","publishdate":"2020-03-09T00:00:00Z","relpermalink":"/project/concrete-compressive-strength/","section":"project","summary":"Predicting compressive strength of Concrete using Machine Learning.","tags":["Machine Learning","Industry 4.0","Civil Engineering"],"title":"Concrete Compressive Strength Prediction","type":"project"},{"authors":["Pranay Modukuru"],"categories":[],"content":"\rView post on Medium.\n\r\rWe can observe a high positive correlation between **compressive Strength** (CC_Strength) and **Cement**. this is true because strength concrete indeed increases with an increase in amount of cement used in preparing it. Also, **Age** and **Super Plasticizer** are other two factors influencing Compressive strength.\rThere are other strong correlations between the fetures,\r* A strong negative correlation between **Super Plasticizer** and **Water**.\r* positive correlations between **Super Plasticizer** and **Fly Ash**, **Fine Aggregate**.\rThese correlations are useful to understand the data in detail, as they give an idea about how a variable is affecting the other. We can further use a **pairplot** in seaborn to plot pair wise relations between all the features and distributions of features along the diagonal.\r```python\rsns.pairplot(data)\r```\r\r\rThe pair plot gives a visual representation of correlations between all the features.\rWe can plot scatter plots between **CC_Strength** and other features to see more complex relations.\r##### CC_Strength vs (Cement, Age, Water)\r```python\rsns.scatterplot(y=\"CC_Strength\", x=\"Cement\", hue=\"Water\",\rsize=\"Age\", data=data, ax=ax, sizes=(50, 300))\r```\r\r\rThe observations we can make from this plot,\r* **Compressive strength increases as amount of cement increases**, as the dots move up when we move towards right on the x-axis.\r* **Compressive strength increases with age** (as the size of dots represents the age), this not the case always but can be up to an extent. * **Cement with less age requires more cement for higher strength**, as the smaller dots are moving up when we move towards right on x-axis.\r* **The older the cement is the more water it requires**, can be confirmed by observing the colour of the dots. Larger dots with dark colour indicate high age and more water. * **Concrete strength increases when less water is used** in preparing it, since the dots on the lower side (y-axis) are darker and the dots on higher end (y-axis) are brighter.\r##### CC Strength vs (Fine aggregate, Super Plasticizer, Fly Ash)\r```python\rsns.scatterplot(y=\"CC_Strength\", x=\"FineAggregate\", hue=\"FlyAsh\", size=\"Superplasticizer\",\rdata=data, ax=ax, sizes=(50, 300))\r```\r\r\rObservations,\r* **Compressive strength decreases Fly ash increases**, as more darker dots are concentrated in the region representing low compressive strength.\r* **Compressive strength increases with Super plasticizer**, since larger the dot the higher they are in the plot.\rWe can visually understand 2D, 3D and max up to 4D plots (features represented by colour and size) as shown above, we can further use row wise and column wise plotting features by seaborn to do further analysis, but still we lack the ability to track all these correlations by ourselves. For this reason, we can turn to Machine Learning to capture these relations and give better insights into the problem.\r### Data preprocessing\rBefore we fit machine learning models on the data, we need to split the data into train, test splits. The features can be rescaled to have a mean of zero and a standard deviation of 1 i.e. all the features fall into the same range.\r```python\rX = data.iloc[:,:-1] # Features\ry = data.iloc[:,-1] # Target\rX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\rsc = StandardScaler()\rX_train = sc.fit_transform(X_train)\rX_test = sc.transform(X_test)\r```\r### Model Building\rAfter preparing the data, we can fit different models on the training data and compare their performance to choose the algorithm with good performance. As this is a regression problem, we can use RMSE (Root Mean Square Error) and $R^2$ score as evaluation metrics.\r#### 1. Linear Regression\rWe will start with Linear Regression, since this is the go-to algorithm for any regression problem. The algorithm tries to form a linear relationship between the input features and the target variable i.e. it fits a straight line given by, $$y = W*X + b = \\sum_{i=1}^{n} w_i * x_i + b$$ Where $w_i$ corresponds to the coefficient of feature $x_i$.\rThe magnitude of these coefficients can be further controlled by using regularization terms to the cost functions. Adding the sum of the magnitudes of the coefficients will result in the coefficients being close to zero, this variation of linear regression is called **Lasso** Regression. Adding the sum of squares of the coefficients to the cost function will make the coefficients be in the same range and this variation is called **Ridge** Regression. Both these variations help in reducing the model complexity and therefore reducing the chances of overfitting on the data.\r```python\r# Importing models\rfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\r# Linear Regression\rlr = LinearRegression()\r# Lasso Regression\rlasso = Lasso()\r# Ridge Regression\rridge = Ridge()\r# Fitting models on Training data\rlr.fit(X_train, y_train)\rlasso.fit(X_train, y_train)\rridge.fit(X_train, y_train)\r# Making predictions on Test data\ry_pred_lr = lr.predict(X_test)\ry_pred_lasso = lasso.predict(X_test)\ry_pred_ridge = ridge.predict(X_test)\rfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\rprint(\"Model\\t\\t\\t RMSE \\t\\t R2\")\rprint(\"\"\"LinearRegression \\t {:.2f} \\t\\t{:.2f}\"\"\".format(\rnp.sqrt(mean_squared_error(y_test, y_pred_lr)), r2_score(y_test, y_pred_lr)))\rprint(\"\"\"LassoRegression \\t {:.2f} \\t\\t{:.2f}\"\"\".format(\rnp.sqrt(mean_squared_error(y_test, y_pred_lasso)), r2_score(y_test, y_pred_lasso)))\rprint(\"\"\"RidgeRegression \\t {:.2f} \\t\\t{:.2f}\"\"\".format(\rnp.sqrt(mean_squared_error(y_test, y_pred_ridge)), r2_score(y_test, y_pred_ridge)))\r```\r###### Output\r| Model\t| RMSE |\tR2 |\r| ---------------- | ------ | ------ |\r| LinearRegression |\t10.29 |\t0.57 |\r| LassoRegression |\t10.68 |\t0.54 |\r| RidgeRegression | 10.29 |\t0.57 |\rThere is not much difference between the performance with these three algorithms, we can plot the coefficients assigned by the three algorithms for the features with the following code.\r```python\rcoeff_lr = lr.coef_\rcoeff_lasso = lasso.coef_\rcoeff_ridge = ridge.coef_\rlabels = req_col_names[:-1]\rx = np.arange(len(labels))\rwidth = 0.3\rfig, ax = plt.subplots(figsize=(10,6))\rrects1 = ax.bar(x - 2*(width/2), coeff_lr, width, label='LR')\rrects2 = ax.bar(x, coeff_lasso, width, label='Lasso')\rrects3 = ax.bar(x + 2*(width/2), coeff_ridge, width, label='Ridge')\rax.set_ylabel('Coefficient')\rax.set_xlabel('Features')\rax.set_title('Feature Coefficients')\rax.set_xticks(x)\rax.set_xticklabels(labels, rotation=45)\rax.legend()\rdef autolabel(rects):\r\"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\rfor rect in rects:\rheight = rect.get_height()\rax.annotate('{:.2f}'.format(height), xy=(rect.get_x() + rect.get_width() / 2, height),\rxytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\rautolabel(rects1)\rautolabel(rects2)\rautolabel(rects3)\rfig.tight_layout()\rplt.show()\r```\r\r\rAs seen in the figure, Lasso regression pushes the coefficients towards zero and the coefficients with the normal Linear Regression and Ridge Regression are almost the same.\rWe can further see how the predictions are by plotting the true values and predicted values,\r```python\rfig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(12,4))\rax1.scatter(y_pred_lr, y_test, s=20)\rax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\rax1.set_ylabel(\"True\")\rax1.set_xlabel(\"Predicted\")\rax1.set_title(\"Linear Regression\")\rax2.scatter(y_pred_lasso, y_test, s=20)\rax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\rax2.set_ylabel(\"True\")\rax2.set_xlabel(\"Predicted\")\rax2.set_title(\"Lasso Regression\")\rax3.scatter(y_pred_ridge, y_test, s=20)\rax3.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\rax3.set_ylabel(\"True\")\rax3.set_xlabel(\"Predicted\")\rax3.set_title(\"Ridge Regression\")\rfig.suptitle(\"True vs Predicted\")\rfig.tight_layout(rect=[0, 0.03, 1, 0.95])\r```\r\r\rIf the predicted values and the target values are equal, then the points on the scatter plot will lie on the straight line. As we can see here, non of the model predicts the Compressive Strength correctly.\r#### 2. Decision Trees\rA Decision Tree Algorithm represents the data with a tree like structure, where each node represents a decision taken on a feature. This algorithm would give better performance in this case, since we have a lot of zeros in some of the input features as seen from their distributions in the pair plot above. This would help the decision trees build trees based on some conditions on features which can further improve performance.\r```python\rfrom sklearn.tree import DecisionTreeRegressor\rdtr = DecisionTreeRegressor()\rdtr.fit(X_train, y_train)\ry_pred_dtr = dtr.predict(X_test)\rprint(\"Model\\t\\t\\t\\t RMSE \\t\\t R2\")\rprint(\"\"\"Decision Tree Regressor \\t {:.2f} \\t\\t{:.2f}\"\"\".format(\rnp.sqrt(mean_squared_error(y_test, y_pred_dtr)), r2_score(y_test, y_pred_dtr)))\rplt.scatter(y_test, y_pred_dtr)\rplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\rplt.xlabel(\"Predicted\")\rplt.ylabel(\"True\")\rplt.title(\"Decision Tree Regressor\")\rplt.show()\r```\r| Model\t| RMSE | R2 |\r| ----------------------- | ------ | ----- |\r| Decision Tree Regressor |\t7.31 |\t0.78 |\r\r\rThe Root Mean Squared Error (RMSE) has come down from 10.29 to 7.31, so the Decision Tree Regressor has improved the performance by a significant amount. This can be observed in the plot as well as more points are closer to the line.\r#### 3. Random Forests\rSince Using a Decision Tree Regressor has improved our performance, we can further improve the performance by ensembling more trees. Random Forest Regressor trains randomly initialized trees with random subsets of data sampled from the training data, this will make our model more robust.\r```python\rfrom sklearn.ensemble import RandomForestRegressor\rrfr = RandomForestRegressor(n_estimators=100)\rrfr.fit(X_train, y_train)\ry_pred_rfr = rfr.predict(X_test)\rprint(\"Model\\t\\t\\t\\t RMSE \\t\\t R2\")\rprint(\"\"\"Random Forest Regressor \\t {:.2f} \\t\\t{:.2f}\"\"\".format(\rnp.sqrt(mean_squared_error(y_test, y_pred_rfr)), r2_score(y_test, y_pred_rfr)))\rplt.scatter(y_test, y_pred_rfr)\rplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\rplt.xlabel(\"Predicted\")\rplt.ylabel(\"True\")\rplt.title(\"Random Forest Regressor\")\rplt.show()\r```\r| Model\t| RMSE | R2 |\r| ----------------------- | ------ | ----- |\r| Random Forest Regressor |\t5.08 |\t0.89 |\r\r\rThe RMSE has further reduced by ensembling multiple trees. We can plot the feature importance's for tree based models. The feature importance's show how important a feature is for a model when making a prediction.\r```python\rfeature_dtr = dtr.feature_importances_\rfeature_rfr = rfr.feature_importances_\rlabels = req_col_names[:-1]\rx = np.arange(len(labels))\rwidth = 0.3\rfig, ax = plt.subplots(figsize=(10,6))\rrects1 = ax.bar(x-(width/2), feature_dtr, width, label='Decision Tree')\rrects2 = ax.bar(x+(width/2), feature_rfr, width, label='Random Forest')\rax.set_ylabel('Importance')\rax.set_xlabel('Features')\rax.set_title('Feature Importance')\rax.set_xticks(x)\rax.set_xticklabels(labels, rotation=45)\rax.legend(loc=\"upper left\", bbox_to_anchor=(1,1))\rautolabel(rects1)\rautolabel(rects2)\rfig.tight_layout()\rplt.show()\r```\r\r\rCement and Age are treated as the most important features by tree based models. Fly ash, Coarse and Fine aggregates are least important factors when predicting the strength of Concrete.\r#### Comparison\rFinally, lets compare the results of all the algorithms.\r```python\rmodels = [lr, lasso, ridge, dtr, rfr]\rnames = [\"Linear Regression\", \"Lasso Regression\", \"Ridge Regression\",\r\"Decision Tree Regressor\", \"Random Forest Regressor\"]\rrmses = []\rfor model in models:\rrmses.append(np.sqrt(mean_squared_error(y_test, model.predict(X_test))))\rx = np.arange(len(names))\rwidth = 0.3\rfig, ax = plt.subplots(figsize=(10,7))\rrects = ax.bar(x, rmses, width)\rax.set_ylabel('RMSE')\rax.set_xlabel('Models')\rax.set_title('RMSE with Different Algorithms')\rax.set_xticks(x)\rax.set_xticklabels(names, rotation=45)\rautolabel(rects)\rfig.tight_layout()\rplt.show()\r```\r\r\r### Conclusion\rWe have analysed the Compressive Strength Data and used Machine Learning to predict the Compressive Strength of Concrete. We have used Linear Regression and its variations, Decision Trees and Random Forests to make predictions and compared their performance. Random Forest Regressor has the lowest RMSE and is a good choice for this problem. Also, we can further improve the performance of the algorithm by tuning the hyperparameters by performing a grid search or random search. ### References\r1. I-Cheng Yeh, \"[Modeling of strength of high performance concrete using artificial neural networks](https://www.sciencedirect.com/science/article/abs/pii/S0008884698001653),\" Cement and Concrete Research, Vol. 28, No. 12, pp. 1797-1808 (1998).\r2. Ahsanul Kabir, Md Monjurul Hasan, Khasro Miah, \"[Strength Prediction Model for Concrete](https://www.researchgate.net/publication/258255660_Strength_Prediction_Model_for_Concrete)\", ACEE Int. J. on Civil and Environmental Engineering, Vol. 2, No. 1, Aug 2013.\r3. https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength --\r","date":1583369632,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583369632,"objectID":"33be358ec095b0a6967d73d82fb03ec1","permalink":"https://pranaymodukuru.github.io/post/concrete-compressive-strength/","publishdate":"2020-03-05T01:53:52+01:00","relpermalink":"/post/concrete-compressive-strength/","section":"post","summary":"The purpose of this post is to demonstrate the use of Machine learning as a tool for Civil Engineering","tags":["Machine Learning","Data Analysis","Data Visualization","Industry 4.0"],"title":"Concrete Compressive Strength Prediction using Machine Learning","type":"post"},{"authors":null,"categories":null,"content":"Removing noise from images has been a reasonably tough task until the deep learning based auto encoders transformed the image processing field. I used a Deep Convolutional Autoencoder to remove coffe stains, footprints, marks resulting from folding or wrinkles from scanned office documents.\nPlease click on links below for more details  \rProject \rCode  ","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"b1f360d9383396416b3dd170d31c1d9f","permalink":"https://pranaymodukuru.github.io/project/denoising-auto-encoder/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/project/denoising-auto-encoder/","section":"project","summary":"Removing noise/dirt marks from scanned documents using Deep Autoencoder.","tags":["Deep Learning","Machine Learning","Autoencoder","Denoising","Image processing"],"title":"Denoising Autoencoder","type":"project"},{"authors":null,"categories":null,"content":"Gesture Recognition is an important application in many domains. For example:\n In games, to enable the player to control game elements with hand In cars, for touch-less dashboards and enhanced safety In Language, identifying sign languages \u0026hellip;  In this project, I collected hand gesture images from my laptop\u0026rsquo;s webcam and trained a Convolutional Neural Network (CNN) to recognize these gestures.\nMy future plan is to use the trained CNN in a game, to predict the hand gestures in real-time.\nPlease click on links below for more details  \rProject \rCode  ","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"58eb9bbf2d45fccfd9b3d93582e8d8c6","permalink":"https://pranaymodukuru.github.io/project/hand-gesture-recognition/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/project/hand-gesture-recognition/","section":"project","summary":"Recognizing Hand Gestures with Convolutional Neural Networks.","tags":["Deep Learning","Machine Learning","Gesture Recognition"],"title":"Hand Gesture Recognition","type":"project"},{"authors":null,"categories":null,"content":"Predicting Rotor Temperature of a Permanent Magnet Synchronous Motor(PMSM) using a Convolutional Neural Network(CNN) The rotor temperature of any motor is difficult to measure as it is a rotating part. Placing any sensors to measure this difficult to measure temperature would result in increase in costs and also increase the weight of the motor. In the era of electric vehicles, electric drives have become common in automotives and a lot of research is ongoing to reduce the weight of the motors in order to increase the efficiency of electric cars.\nMeasurement of quantities like temperature, torque of the rotor is important in order to design control systems to effectively control the motor. Many statistical based approaches have been studied in estimating the values of temperatures and torque, but these approaches require domain knowledge and often are different for different motors and different operating conditions. There is no universal approach towards estimating these values.\nWith the advent of Deep Learning, methods have been proposed to use deep learning approaches to predict the sensor values. The goal of the project is to efficiently predict the rotor temperature of a permanent magnet synchronous motor (PMSM), as it is usually difficult to measure the rotor temperature. This kind of prediction helps to reduce the amount of equipment that is to be mounted on to the motor to measure the temperature.\nPlease click on links below for more details  \rProject \rCode  ","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"c408771c433616ff6f4ba074e750692a","permalink":"https://pranaymodukuru.github.io/project/pmsm-rotor-temperature/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/project/pmsm-rotor-temperature/","section":"project","summary":"Predicting rotor temperature of a Permanent Magnet Synchronous Motor (PMSM) with Convolutional Neural Networks.","tags":["Deep Learning","Machine Learning","Industry 4.0","Soft Sensor"],"title":"PMSM Rotor Temperature Prediction","type":"project"}]